# 文字识别 Optical Character Recognition,OCR

[自然场景文本检测识别技术综述](https://cloud.tencent.com/developer/article/1154619)

将图片上的文字内容，智能识别成为可编辑的文本。

> 场景文字识别（Scene Text Recognition，STR）

OCR（Optical Character Recognition, 光学字符识别）传统上指对输入扫描文档图像进行分析处理，识别出图像中文字信息。场景文字识别（Scene Text Recognition，STR） 指识别自然场景图片中的文字信息。自然场景图像中的文字识别，其难度远大于扫描文档图像中的文字识别，因为它的文字展现形式极其丰富：

1·允许多种语言文本混合，字符可以有不同的大小、字体、颜色、亮度、对比度等。  
2·文本行可能有横向、竖向、弯曲、旋转、扭曲等式样。  
3·图像中的文字区域还可能会产生变形(透视、仿射变换)、残缺、模糊等现象。  
4·自然场景图像的背景极其多样。如文字可以出现在平面、曲面或折皱面上；文字区域附近有复杂的干扰纹理、或者非文字区域有近似文字的纹理，比如沙地、草丛、栅栏、砖墙等。  


也有人用OCR技术泛指所有图像文字检测和识别技术， 包括传统OCR技术与场景文字识别技术。这是因为，场景文字识别技术可以被看成是传统OCR技术的自然演进与升级换代。



> **应用:**

1.身份证、名片、银行卡、户口本等卡证类、出版物(扫描版图像、试题)、票据类(发票、火车票、彩票、出租车票)的印刷体识别；  

2.运单、考试试卷、办公手写文档、快递手写单号等手写体识别；  

3.车牌、集装箱号、快递运单、行驶证、驾驶证、等交通物流字符识别等；  

4.水表、电表、燃气表等各种传感器可视化数据识别(5G物联网之后可能就不需要了);  

5.图像文字检测和识别技术有着广泛的应用场景。已经被互联网公司落地的相关应用涉及了识别名片、识别菜单、识别快递单、识别身份证、识别营业证、识别银行卡、识别车牌、识别路牌、识别商品包装袋、识别会议白板、识别广告主干词、识别试卷、识别单据等等。  

文本检测和识别技术处于一个学科交叉点，其技术演进不断受益于计算机视觉处理和自然语言处理两个领域的技术进步。它既需要使用视觉处理技术来提取图像中文字区域的图像特征向量，又需要借助自然语言处理技术来解码图像特征向量为文字结果。  


## 什么是OCR？

OCR英文全称是Optical Character Recognition，中文叫做光学字符识别。它是利用光学技术和计算机技术把印在或写在纸上的文字读取出来，并转换成一种计算机能够接受、人又可以理解的格式。文字识别是计算机视觉研究领域的分支之一，而且这个课题已经是比较成熟了，并且在商业中已经有很多落地项目了。比如汉王OCR、百度OCR、阿里OCR、腾讯OCR等等，很多企业都有能力都是拿OCR技术开始挣钱了。其实我们自己也能感受到，OCR技术确实也在改变着我们的生活：比如一个手机APP就能帮忙扫描名片、身份证，并识别出里面的信息；汽车进入停车场、收费站都不需要人工登记了，都是用车牌识别技术；我们看书时看到不懂的题，拿个手机一扫，APP就能在网上帮你找到这题的答案。太多太多的应用了，OCR的应用在当今时代确实是百花齐放啊。

## OCR的发展

在一些简单环境下OCR的准确度已经比较高了（比如电子文档），但是在一些复杂环境下的字符识别，在当今还没有人敢说自己能做的很好。现在大家都很少会把目光还放在如何对电子文档的文字识别该怎么进一步提高准确率了，因为他们把目光放在更有挑战性的领域。OCR传统方法在应对复杂图文场景的文字识别显得力不从心，越来越多人把精力都放在研究如何把文字在复杂场景读出来，并且读得准确作为研究课题，用学界术语来说，就是场景文本识别（文字检测+文字识别）。自然场景下的文字识别比简单场景的文字识别实在困难太多了，现在虽然出了很多成果，但是离理想结果还是差很远。


## OCR的分类

如果要给OCR进行分类，我觉得可以分为两类：**手写体识别和印刷体识别**。这两个可以认为是OCR领域两个大主题了，当然印刷体识别较手写体识别要简单得多，我们也能从直观上理解，印刷体大多都是规则的字体，因为这些字体都是计算机自己生成再通过打印技术印刷到纸上。在印刷体的识别上有其独特的干扰：在印刷过程中字体很可能变得断裂或者墨水粘连，使得OCR识别异常困难。当然这些都可以通过一些图像处理的技术帮他尽可能的还原，进而提高识别率。总的来说，单纯的印刷体识别在业界已经能做到很不错了，但说100%识别是肯定不可能的，但是说识别得不错那是没毛病。

印刷体已经识别得不错了，那么手写体呢？手写体识别一直是OCR界一直想攻克的难关，但是时至今天，感觉这个难关还没攻破，还有很多学者和公司在研究。为什么手写体识别这么难识别？因为人类手写的字往往带有个人特色，每个人写字的风格基本不一样，虽然人类可以读懂你写的文字，但是机器缺很难。那为什么机器能读懂印刷体？因为印刷体是机器造出来的啊，那机器当然能读懂自己造的字体啦哈哈~其实上面也提到了，印刷体一般都比较规则，字体都基本就那几十种，机器学习这几十种字体并不是一件难事，但是手写体，每个人都有一种字体的话，那机器该学习多少字体啊？这就是难度所在。

如果按识别的内容来分类，也就是按照识别的语言的分类的话，那么要识别的内容将是人类的所有语言**（汉语、英语、德语、法语等）**。如果仅按照我们国人的需求，那识别的内容就包括：**汉字、英文字母、阿拉伯数字、常用标点符号**。根据要识别的内容不同，识别的难度也各不相同。简单而言，识别数字是最简单了，毕竟要识别的字符只有0~9，而英文字母识别要识别的字符有26个（如果算上大小写的话那就52个），而中文识别，要识别的字符高达数千个（二级汉字一共6763个）！因为汉字的字形各不相同，结构非常复杂（比如带偏旁的汉字）如果要将这些字符都比较准确地识别出来，是一件相当具有挑战性的事情。但是，并不是所有应用都需要识别如此庞大的汉字集，比如车牌识别，我们的识别目标仅仅是数十个中国各省和直辖市的简称，难度就大大减少了。当然，在一些文档自动识别的应用是需要识别整个汉字集的，所以要保证识别的整体的识别还是很困难的。

传统OCR一般有 模板匹配的方法(简单的场景 单一数字识别)、特征设计提取分类(传统机器学习方法)

## 现代 OCR 流程
深度学习的出现，让OCR技术焕发第二春。现在OCR基本都用卷积神经网络来做了，而且识别率也是惊人的好，人们也不再需要花大量时间去设计字符特征了。在OCR系统中，人工神经网络主要充当特征提取器和分类器的功能，输入是字符图像，输出是识别结果，一气呵成。

* **1.图像预处理(做角度矫正和去噪) **

[传统opencv 轮廓检测+透视变换+二值化](https://www.cnblogs.com/skyfsm/p/7324346.html)

最后总结一下两个算法的应用场景：

> 基于轮廓提取的矫正算法更适用于车牌、身份证、人民币、书本、发票一类矩形形状而且边界明显的物体矫正。

> 基于直线探测的矫正算法更适用于文本类的矫正。

[基于轮廓和直线的图片校正](https://www.cnblogs.com/skyfsm/p/6902524.html)

[cnn计算图像透视变换系数 Spatial Transformer Network(STN) ](https://arxiv.org/pdf/1506.02025.pdf)

对于弯曲不规则文本，如果按照之前的识别方法，直接将整个文本区域图像强行送入CNN+RNN，由于有大量的无效区域会导致识别效果很差。所以这篇文章提出一种通过**STN网络Spatial Transformer Network(STN)**学习变换参数，将Rectified Image对应的特征送入后续RNN中识别。

对于STN网络，可以学习一组点 (x_i^s,y_i^s) 到对应点 (x_i^t,y_i^t) 的变换。而且STN可以插入轻松任意网络结构中学习到对应的变换。

    (x_i^s,
    y_i^s)    =  (c11, c12, c13
                  c21, c22, c23)    *  (x_i^t,
                                        y_i^t,
                                        1) 
                                        
〉* **2.字符检测(行分割/列分 解决的问题是哪里有文字，文字的范围)**

传统算法：行切割（水平投影依据像素值(0为黑色)判断行起止）+列切割（垂直投影）
    
    比如“刺”字被分为两部分了，那么我们就直接将这两个“字”送去识别，结果当然是得到一个置信度很低的一个反馈，那么我们就将这两个部分往他们身边最近的、而且没被成功识别的部分进行合并，再将这个合并后的字送进OCR识别，这样子我们就可以通过识别反馈来完成汉字的正确分割和识别了。
    
目标检测相关算法：yolo/ssd/frcnn
    
    准确度还比较高

〉* **3.字符识别(单个字符识别/序列字符识别)**

对定位好的文字区域进行识别，主要解决的问题是每个文字是什么，将图像中的文字区域进转化为字符信息.  

cnn + rnn(lstm) + attention

cnn + rnn(lstm) + CTC

现今基于深度学习的端到端OCR技术有两大主流技术：CRNN OCR和attention OCR。其实这两大方法主要区别在于最后的输出层（翻译层），即怎么将网络学习到的序列特征信息转化为最终的识别结果。这两大主流技术在其特征学习阶段都采用了CNN+RNN的网络结构，CRNN OCR在对齐时采取的方式是CTC算法（应用更为广泛），而attention OCR采取的方式则是attention机制。


〉* **4.后处理识别矫正(语法检测器，检测字符的组合逻辑是否合理)**




# 基础网络 

图文识别任务中充当特征提取模块的基础网络，可以来源于通用场景的图像分类模型。例如，VGGNet，ResNet、InceptionNet、DenseNet、Inside-Outside Net、Se-Net等。

图文识别任务中的基础网络，也可以来源于特定场景的专用网络模型。

例如，**擅长提取图像细节特征的FCN网络，**

**擅长做图形矫正的STN网络。**

## FCN网络

全卷积网络（FCN,fully convolutional network）， 是去除了全连接(fc)层的基础网络，最初是用于实现语义分割任务。FC的优势在于利用反卷积（deconvolution）、上池化（unpooling）等上采样（upsampling）操作，将特征矩阵恢复到接近原图尺寸，然后对每一个位置上的像素做类别预测，从而能识别出更清晰的物体边界。基于FCN的检测网络，不再经过候选区域回归出物体边框, 而是根据高分辨率的特征图直接预测物体边框。因为不需要像Faster-RCNN那样在训练前定义好候选框长宽比例，FCN在预测不规则物体边界时更加鲁棒。由于FCN网络最后一层特征图的像素分辨率较高，而图文识别任务中需要依赖清晰的文字笔画来区分不同字符（特别是汉字），所以FCN网络很适合用来提取文本特征。当FCN被用于图文识别任务时，最后一层特征图中每个像素将被分成**文字行（前景）和非文字行（背景）两个类别。**


## STN网络

空间变换网络（STN，Spatial Transformer Networks）的作用是对输入特征图进行空间位置矫正得到输出特征图，这个矫正过程是可以进行梯度传导的，从而能够支持端到端的模型训练。

如下图所示，STN网络由定位网络（Localization Network） ，网格生成器（Grid generator），采样器（Sampler）共3个部分组成。定位网络根据原始特征图U计算出一套控制参数，网格生成器这套控制参数产生采样网格（sampling grid），采样器根据采样网格核函数将原始图U中像素对应采样到目标图V中。

空间变换的控制参数是根据原始特征图U动态生成的，生成空间变换控制参数的元参数则是在模型训练阶段学习到的、并且存放于定位网络的权重（weights）矩阵中。

## CTC网络
链结式时间分类算法（CTC，Connectionist Temporal Classification）是一个损失函数，主要在序列数据上进行监督式学习，且不需要调整输入数据和标签。

## attention注意力网络 特征权重网络


# 检测网络框架

Faster RCNN作为一个检测网络框架，其目标是寻找紧凑包围被检测对象的边框（BBOX，Bounding Box）。如下图所示，它在Fast RCNN检测框架基础上引入区域建议网络（RPN，Region Proposal Network），来快速产生与目标物体长宽比例接近的多个候选区域参考框（anchor）；它通过ROI（Region of Interest） Pooling层为多种尺寸参考框产生出归一化固定尺寸的区域特征；它利用共享的CNN卷积网络同时向上述RPN网络和ROI Pooling层输入特征映射（Feature Maps），从而减少卷积层参数量和计算量。训练过程中使用到了多目标损失函数，包括RPN网络、ROI Pooling层的边框分类loss和坐标回归loss。通过这些loss的梯度反向传播，能够调节候选框的坐标、并增大它与标注对象边框的重叠度/交并比(IOU，Intersection over Union）。RPN网格生成的候选框初始值有固定位置以及长宽比例。如果候选框初始长宽比例设置得与图像中物体形状差别很大，就很难通过回归找到一个紧凑包围它的边框。


SSD（Single Shot MultiBox Detector），是2016年提出的一种全卷积目标检测算法，截止到目前仍是主要的目标检测框架之一，相比Faster RCNN有着明显的速度优势。如下图所示，SSD是一种one stage算法，直接预测被检测对象的边框和得分。检测过程中，SSD算法利用多尺度思想进行检测，在不同尺度的特征图(feature maps)上产生与目标物体长宽比例接近的多个默认框(Default boxes)，进行回归与分类。最后利用非极大值抑制(Non-maximum suppression)得到最终的检测结果。训练过程中，SSD采用Hard negative mining策略进行训练，使正负样本比例保持为1：3，同时使用多种数据增广(Data augmentation)方式进行训练，提高模型性能。


# 文本检测模型

文本检测模型的目标是从图片中尽可能准确地找出文字所在区域。

但是，视觉领域常规物体检测方法(SSD, YOLO, Faster-RCNN等)直接套用于文字检测任务效果并不理想， 主要原因如下：

1·相比于常规物体，文字行长度、长宽比例变化范围很大。   
2·文本行是有方向性的。常规物体边框BBox的四元组描述方式信息量不充足。   
3·自然场景中某些物体局部图像与字母形状相似，如果不参考图像全局信息将有误报。   
4·有些艺术字体使用了弯曲的文本行，而手写字体变化模式也很多。   
5·由于丰富的背景图像干扰，手工设计特征在自然场景文本识别任务中不够鲁棒。   

针对上述问题根因，近年来出现了各种基于深度学习的技术解决方案。它们从特征提取、区域建议网络(RPN)、多目标协同训练、Loss改进、非极大值抑制（NMS）、半监督学习等角度对常规物体检测方法进行改造，极大提升了自然场景图像中文本检测的准确率。例如：

1·CTPN方案中，用BLSTM模块提取字符所在图像上下文特征，以提高文本块识别精度。   
2·RRPN等方案中，文本框标注采用BBOX +方向角度值的形式，模型中产生出可旋转的文字区域候选框，并在边框回归计算过程中找到待测文本行的倾斜角度。   
3·DMPNet等方案中，使用四边形（非矩形）标注文本框，来更紧凑的包围文本区域。   
4·SegLink  将单词切割为更易检测的小文字块，再预测邻近连接将小文字块连成词。  
5·TextBoxes等方案中，调整了文字区域参考框的长宽比例，并将特征层卷积核调整为长方形，从而更适合检测出细长型的文本行。  
6·FTSN方案中，作者使用Mask-NMS代替传统BBOX的NMS算法来过滤候选框。   
7·WordSup方案中，采用半监督学习策略，用单词级标注数据来训练字符级文本检测模型。  

### CTPN模型

CTPN是目前流传最广、影响最大的开源文本检测模型，可以检测水平或微斜的文本行。文本行可以被看成一个字符sequence，而不是一般物体检测中单个独立的目标。同一文本行上各个字符图像间可以互为上下文，在训练阶段让检测模型学习图像中蕴含的这种上下文统计规律，可以使得预测阶段有效提升文本块预测准确率。CTPN模型的图像预测流程中，前端使用当时流行的VGG16做基础网络来提取各字符的局部图像特征，中间使用BLSTM层提取字符序列上下文特征，然后通过FC全连接层，末端经过预测分支输出各个文字块的坐标值和分类结果概率值。在数据后处理阶段，将合并相邻的小文字块为文本行。

### RRPN模型

基于旋转区域候选网络（RRPN, Rotation Region Proposal Networks）的方案，将旋转因素并入经典区域候选网络（如Faster RCNN）。这种方案中，一个文本区域的ground truth被表示为具有5元组(x,y,h,w,θ)的旋转边框, 坐标(x,y)表示边框的几何中心, 高度h设定为边框的短边，宽度w为长边，方向是长边的方向。训练时，首先生成含有文本方向角的倾斜候选框，然后在边框回归过程中学习文本方向角。

RRPN中方案中提出了旋转感兴趣区域（RRoI，Rotation Region-of-Interest）池化层，将任意方向的区域建议先划分成子区域，然后对这些子区域分别做max pooling、并将结果投影到具有固定空间尺寸小特征图上。

### FTSN模型
FTSN（Fused Text Segmentation Networks）模型使用分割网络支持倾斜文本检测。它使用Resnet-101做基础网络，使用了多尺度融合的特征图。标注数据包括文本实例的像素掩码和边框，使用像素预测与边框检测多目标联合训练。

基于文本实例间像素级重合度的Mask-NMS， 替代了传统基于水平边框间重合度的NMS算法。下图左边子图是传统NMS算法执行结果，中间白色边框被错误地抑制掉了。下图右边子图是Mask-NMS算法执行结果， 三个边框都被成功保留下来。

### DMPNet模型

DMPNet（Deep Matching Prior Network）中，使用四边形（非矩形）来更紧凑地标注文本区域边界，其训练出的模型对倾斜文本块检测效果更好。

如下图所示，它使用滑动窗口在特征图上获取文本区域候选框，候选框既有正方形的、也有倾斜四边形的。接着，使用基于像素点采样的Monte-Carlo方法，来快速计算四边形候选框与标注框间的面积重合度。然后，计算四个顶点坐标到四边形中心点的距离，将它们与标注值相比计算出目标loss。文章中推荐用Ln loss来取代L1、L2 loss，从而对大小文本框都有较快的训练回归（regress）速度。

### EAST模型

EAST（Efficient and Accuracy Scene Text detection pipeline）模型中，首先使用全卷积网络（FCN）生成多尺度融合的特征图，然后在此基础上直接进行像素级的文本块预测。该模型中，支持旋转矩形框、任意四边形两种文本区域标注形式。对应于四边形标注，模型执行时会对特征图中每个像素预测其到四个顶点的坐标差值。对应于旋转矩形框标注，模型执行时会对特征图中每个像素预测其到矩形框四边的距离、以及矩形框的方向角。

根据开源工程中预训练模型的测试，该模型检测英文单词效果较好、检测中文长文本行效果欠佳。或许，根据中文数据特点进行针对性训练后，检测效果还有提升空间。

上述过程中，省略了其他模型中常见的区域建议、单词分割、子块合并等步骤，因此该模型的执行速度很快。

### SegLink模型

SegLink模型的标注数据中，先将每个单词切割为更易检测的有方向的小文字块（segment），然后用邻近连接（link ）将各个小文字块连接成单词。这种方案方便于识别长度变化范围很大的、带方向的单词和文本行，它不会象Faster-RCNN等方案因为候选框长宽比例原因检测不出长文本行。相比于CTPN等文本检测模型，SegLink的图片处理速度快很多。

如下图所示，该模型能够同时从6种尺度的特征图中检测小文字块。同一层特征图、或者相邻层特征图上的小文字块都有可能被连接入同一个单词中。换句话说，位置邻近、并且尺寸接近的文字块都有可能被预测到同一单词中。

### PixelLink模型

自然场景图像中一组文字块经常紧挨在一起，通过语义分割方法很难将它们识别开来，所以PixelLink模型尝试用实例分割方法解决这个问题。

该模型的特征提取部分，为VGG16基础上构建的FCN网络。模型执行流程如下图所示。首先，借助于CNN 模块执行两个像素级预测：一个文本二分类预测，一个链接二分类预测。接着，用正链接去连接邻居正文本像素，得到文字块实例分割结果。然后，由分割结果直接就获得文字块边框， 而且允许生成倾斜边框。

上述过程中，省掉了其他模型中常见的边框回归步骤，因此训练收敛速度更快些。训练阶段，使用了平衡策略，使得每个文字块在总LOSS中的权值相同。训练过程中，通过预处理增加了各种方向角度的文字块实例。

### Textboxes/Textboxes++模型

Textboxes是基于SSD框架的图文检测模型，训练方式是端到端的，运行速度也较快。如下图所示，为了适应文字行细长型的特点，候选框的长宽比增加了1,2,3,5,7,10这样初始值。为了适应文本行细长型特点，特征层也用长条形卷积核代替了其他模型中常见的正方形卷积核。为了防止漏检文本行，还在垂直方向增加了候选框数量。为了检测大小不同的字符块，在多个尺度的特征图上并行预测文本框， 然后对预测结果做NMS过滤。

Textboxes++是Textboxes的升级版本，目的是增加对倾斜文本的支持。为此，将标注数据改为了旋转矩形框和不规则四边形的格式；对候选框的长宽比例、特征图层卷积核的形状都作了相应调整。

### WordSup模型

如下图所示，在数学公式图文识别、不规则形变文本行识别等应用中，字符级检测模型是一个关键基础模块。由于字符级自然场景图文标注成本很高、相关公开数据集稀少，导致现在多数图文检测模型只能在文本行、单词级标注数据上做训练。WordSup提出了一种弱监督的训练框架， 可以文本行、单词级标注数据集上训练出字符级检测模型。

WordSup弱监督训练框架中，两个训练步骤被交替执行：给定当前字符检测模型，并结合单词级标注数据，计算出字符中心点掩码图； 给定字符中心点掩码图，有监督地训练字符级检测模型.

训练好字符检测器后，可以在数据流水线中加入合适的文本结构分析模块，以输出符合应用场景格式要求的文本内容。该文作者例举了多种文本结构分析模块的实现方法。

# 文本识别模型

文本识别模型的目标是从已分割出的文字区域中识别出文本内容。


## CRNN模型

现今基于深度学习的端到端OCR技术有两大主流技术：CRNN OCR和attention OCR。其实这两大方法主要区别在于最后的输出层（翻译层），即怎么将网络学习到的序列特征信息转化为最终的识别结果。这两大主流技术在其特征学习阶段都采用了CNN+RNN的网络结构，CRNN OCR在对齐时采取的方式是CTC算法（应用更为广泛），而attention OCR采取的方式则是attention机制。

[【OCR技术系列之七】端到端不定长文字识别CRNN算法详解](https://www.cnblogs.com/skyfsm/p/10335717.html)

[代码 pytorch + wrap_ctc ](https://github.com/Ewenwan/crnn)

[百度warp-ctc CPU和GPU上高效并行的CTC代码库 （library）](https://github.com/Ewenwan/warp-ctc)

[caffe crnn](https://github.com/Ewenwan/crnn.caffe)

[use STN+CNN+BLSTM+CTC to do OCR](https://github.com/wushilian/STN_CNN_LSTM_CTC_TensorFlow)

CRNN(Convolutional Recurrent Neural Network）是目前较为流行的图文识别模型，可识别较长的文本序列。它包含CNN特征提取层和BLSTM序列特征提取层，能够进行端到端的联合训练。 它利用BLSTM和CTC部件学习字符图像中的上下文关系， 从而有效提升文本识别准确率，使得模型更加鲁棒。预测过程中，前端使用标准的CNN网络提取文本图像的特征，利用BLSTM将特征向量进行融合以提取字符序列的上下文特征，然后得到每列特征的概率分布，最后通过转录层(CTC rule)进行预测得到文本序列。

网络架构。架构包括三部分：

1) 卷积层，使用CNN，作用是从输入图像中提取特征序列；

2) 循环层，使用RNN，作用是预测从卷积层获取的特征序列（每一帧）的标签（真实值）分布;

3) 转录层，使用CTC，作用是把从循环层获取（每一帧）的标签分布通过去重整合等操作转换成最终的识别结果（标签序列）。

![](https://img2018.cnblogs.com/blog/1093303/201901/1093303-20190129201843455-243108334.png)

CNN提取图像像素特征，RNN提取图像时序特征，而CTC归纳字符间的连接特性。

CTC有什么好处？因手写字符的随机性，人工可以标注字符出现的像素范围，但是太过麻烦，ctc可以告诉我们哪些像素范围对应的字符：

在CRNN的底部，卷积层自动从每个输入图像中提取特征序列。在卷积网络之上，构建了一个循环网络，用于对卷积层输出的特征序列的每一帧进行预测。采用CRNN顶部的转录层将循环层的每帧预测转化为标签序列。虽然CRNN由不同类型的网络架构（如CNN和RNN）组成，但可以通过一个损失函数进行联合训练。

> 转录

转录是将RNN所做的每帧预测转换成标签序列的过程。数学上，转录是根据每帧预测找到具有最高概率的标签序列。在实践中，存在两种转录模式，即无词典转录和基于词典的转录。词典是一组标签序列，预测受拼写检查字典约束。在无词典模式中，预测时没有任何词典。在基于词典的模式中，通过选择具有最高概率的标签序列进行预测。

我们采用Graves等人[15]提出的联接时间分类（CTC）层中定义的条件概率。按照每帧预测y=y1,...,yT对标签序列l定义概率，并忽略l中每个标签所在的位置。因此，当我们使用这种概率的负对数似然作为训练网络的目标函数时，我们只需要图像及其相应的标签序列，避免了标注单个字符位置的劳动。

CRNN OCR其实是借用了语音识别中解决不定长语音序列的思路。与语音识别问题类似，OCR可建模为时序依赖的词汇或者短语识别问题。基于联结时序分类(Connectionist Temporal Classification, CTC)训练RNN的算法，在语音识别领域显著超过传统语音识别算法。一些学者尝试把CTC损失函数借鉴到OCR识别中，CRNN 就是其中代表性算法。CRNN算法输入100*32归一化高度的词条图像，基于7层CNN（普遍使用VGG16）提取特征图，把特征图按列切分（Map-to-Sequence），每一列的512维特征，输入到两层各256单元的双向LSTM进行分类。在训练过程中，通过CTC损失函数的指导，实现字符位置与类标的近似软对齐。

CRNN借鉴了语音识别中的LSTM+CTC的建模方法，不同点是输入进LSTM的特征，从语音领域的声学特征（MFCC等），替换为CNN网络提取的图像特征向量。CRNN算法最大的贡献，是把CNN做图像特征工程的潜力与LSTM做序列化识别的潜力，进行结合。它既提取了鲁棒特征，又通过序列识别避免了传统算法中难度极高的单字符切分与单字符识别，同时序列化识别也嵌入时序依赖（隐含利用语料）。在训练阶段，CRNN将训练图像统一缩放100×32（w × h）；在测试阶段，针对字符拉伸导致识别率降低的问题，CRNN保持输入图像尺寸比例，但是图像高度还是必须统一为32个像素，卷积特征图的尺寸动态决定LSTM时序长度。

1.input： 输入文字块，归一化到32*w 即height缩放到32，宽度按高度的比率缩放，也可以缩放到自己想要的宽度，训练时为批次训练，缩放到[32,Wmax]），示例为（32,128）  

2.经过两个conv层和两个poling层，conv3层时数据大小为256*8*32，两个pooling层步长为2.  

3.pooling2层步长为（2，1），（个人看法：作者使用的英文训练，英文字符的特征是高大于宽的特征，倘若使用中文训练，建议使用（2,2），我的代码中默认为（2,2），示例以（2，1）为例,所以此时输出为256*4*33  

4.bn层不改变输出的大小（就是做个归一化，加速训练收敛），p3层时,w+1,所以pooling3层时，输出为512*2*34  

5.conv7层时，kernel 为22，stride(1,1) padding(0,0)

    Wnew = (2 + 2 padW - kernel ) / strideW + 1 = 1
    Hnew = 33
    所以conv7层输出为512133

6. 后面跟两个双向Lstm,隐藏节点都是256

    Blstm1输出33*1256
    Blstm2输出 33*1*5530 5530 = 字符个数 + 非字符 = 5529 + 1

最终的输出结果直观上可以想象成将128分为33份，每一份对应5530个类别的概率

现在输入有个图像，为了将特征输入到Recurrent Layers，做如下处理：

1.首先会将图像缩放到 32×W×1 大小  
2.然后经过CNN后变为 1×（W/4）× 512  
3.接着针对LSTM，设置 T=(W/4) ， D=512 ，即可将特征输入LSTM。   
4.LSTM有256个隐藏节点，经过LSTM后变为长度为T × nclass的向量，再经过softmax处理，列向量每个元素代表对应的字符预测概率，最后再将这个T的预测结果去冗余合并成一个完整识别结果即可。   

> cTc

我们知道，CRNN中RNN层输出的一个不定长的序列，比如原始图像宽度为W，可能其经过CNN和RNN后输出的序列个数为S，此时我们要将该序列翻译成最终的识别结果。RNN进行时序分类时，不可避免地会出现很多冗余信息，比如一个字母被连续识别两次，这就需要一套去冗余机制，但是简单地看到两个连续字母就去冗余的方法也有问题，比如cook，geek一类的词，所以CTC有一个blank机制来解决这个问题。

![](https://img2018.cnblogs.com/blog/1093303/201901/1093303-20190129201921725-1294260731.png)

如上图所示，我们要识别这个手写体图像，标签为“ab”，经过CNN+RNN学习后输出序列向量长度为5，即t0~t4，此时我们要将该序列翻译为最后的识别结果。我们在翻译时遇到的第一个难题就是，5个序列怎么转化为对应的两个字母？重复的序列怎么解决？刚好位于字与字之间的空白的序列怎么映射？这些都是CTC需要解决的问题。

我们从肉眼可以看到，t0,t1,t2时刻都应映射为“a”，t3,t4时刻都应映射为“b”。如果我们将连续重复的字符合并成一个输出的话，即“aaabb”将被合并成“ab”输出。但是这样子的合并机制是有问题的，比如我们的标签图像时“aab”时，我们的序列输出将可能会是“aaaaaaabb”，这样子我们就没办法确定该文本应被识别为“aab”还是“ab”。CTC为了解决这种二义性，提出了插入blank机制，比如我们以“-”符号代表blank，则若标签为“aaa-aaaabb”则将被映射为“aab”，而“aaaaaaabb”将被映射为“ab”。引入blank机制，我们就可以很好地处理了重复字符的问题了。

但我们还注意到，“aaa-aaaabb”可以映射为“aab”，同样地，“aa-aaaaabb”也可以映射为“aab”，也就是说，存在多个不同的字符组合可以映射为“aab”，更总结地说，一个标签存在一条或多条的路径。比如下面“state”这个例子，也存在多条不同路径映射为"state"：

![](https://img2018.cnblogs.com/blog/1093303/201901/1093303-20190129201933672-77661160.png)

上面提到，RNN层输出的是序列中概率矩阵，那么

p(π=−−stta−t−−−e|x,S)=∏T(yt_πt)=(y1−)×(y2−)×(y3s)×(y4t)×(y5t)×(y6a)×(y7−)×(y8t)×(y9−)×(y10−)×(y11−)×(y12e)

其中，y1−y−1表示第一个序列输出“-”的概率，那么对于输出某条路径ππ的概率为各个序列概率的乘积。所以要得到一个标签可以有多个路径来获得，从直观上理解就是，我们输出一张文本图像到网络中，我们需要使得输出为标签L的概率最大化，由于路径之间是互斥的，对于标注序列，其条件概率为所有映射到它的路径概率之和：

![](https://img2018.cnblogs.com/blog/1093303/201901/1093303-20190129201945171-1526153135.png)

其中π∈B−1(l)π∈B−1(l)的意思是，所有可以合并成l的所有路径集合。

这种通过映射B和所有候选路径概率之和的方式使得CTC不需要对原始的输入序列进行准确的切分，这使得RNN层输出的序列长度>label长度的任务翻译变得可能。CTC可以与任意的RNN模型，但是考虑到标注概率与整个输入串有关，而不是仅与前面小窗口范围的片段相关，因此双向的RNN/LSTM模型更为适合。

ctc会计算loss ，从而找到最可能的像素区域对应的字符。事实上，这里loss的计算本质是对概率的归纳：

![](https://img2018.cnblogs.com/blog/1093303/201901/1093303-20190129201956696-854357259.png)

如上图，对于最简单的时序为2的（t0t1）的字符识别，可能的字符为“ａ”，“ｂ”和“－”，颜色越深代表概率越高。我们如果采取最大概率路径解码的方法，一看就是“--”的概率最大，真实字符为空即“”的概率为0.6*0.6=0.36。

但是我们忽略了一点，真实字符为“ａ”的概率不只是”aa” 即0.4*0.4 ,　事实上，“aa”, “a-“和“-a”都是代表“ａ”，所以，输出“ａ”的概率为：

0.4*0.4 + 0.4 * 0.6 + 0.6*0.4 = 0.16+0.24+0.24 = 0.64

所以“ａ”的概率比空“”的概率高！可以看出，这个例子里最大概率路径和最大概率序列完全不同，所以CTC解码通常不适合采用最大概率路径的方法，而应该采用前缀搜索算法解码或者约束解码算法。

通过对概率的计算，就可以对之前的神经网络进行反向传播更新。类似普通的分类，CTC的损失函数O定义为负的最大似然，为了计算方便，对似然取对数。

![]()

![]()

## RARE模型

RARE（Robust text recognizer with Automatic Rectification）模型在识别变形的图像文本时效果很好。如下图所示，模型预测过程中，输入图像首先要被送到一个空间变换网络中做处理，矫正过的图像然后被送入序列识别网络中得到文本预测结果。

空间变换网络内部包含定位网络、网格生成器、采样器三个部件。经过训练后，它可以根据输入图像的特征图动态地产生空间变换网格，然后采样器根据变换网格核函数从原始图像中采样获得一个矩形的文本图像。RARE中支持一种称为TPS（thin-plate splines）的空间变换，从而能够比较准确地识别透视变换过的文本、以及弯曲的文本.

# 端到端模型

端到端模型的目标是一站式直接从图片中定位和识别出所有文本内容来。

## FOTS Rotation-Sensitive Regression

FOTS（Fast Oriented Text Spotting）是图像文本检测与识别同步训练、端到端可学习的网络模型。检测和识别任务共享卷积特征层，既节省了计算时间，也比两阶段训练方式学习到更多图像特征。引入了旋转感兴趣区域（RoIRotate）, 可以从卷积特征图中产生出定向的文本区域，从而支持倾斜文本的识别.

STN-OCR模型

STN-OCR是集成了了图文检测和识别功能的端到端可学习模型。在它的检测部分嵌入了一个空间变换网络（STN）来对原始输入图像进行仿射（affine）变换。利用这个空间变换网络，可以对检测到的多个文本块分别执行旋转、缩放和倾斜等图形矫正动作，从而在后续文本识别阶段得到更好的识别精度。在训练上STN-OCR属于半监督学习方法，只需要提供文本内容标注，而不要求文本定位信息。作者也提到，如果从头开始训练则网络收敛速度较慢，因此建议渐进地增加训练难度。STN-OCR已经开放了工程源代码和预训练模型。
